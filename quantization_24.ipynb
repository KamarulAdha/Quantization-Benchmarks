{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/workspace/data/radar/24'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob(os.path.join(directory, '*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11520"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11520/11520 [08:15<00:00, 23.26it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "\n",
    "# Path to the directory\n",
    "directory = '/workspace/data/radar/24'\n",
    "output_directory = \"/workspace/data/radar_quantized/24_quantized\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Get all files in the directory\n",
    "file_paths = glob.glob(os.path.join(directory, '*'))\n",
    "\n",
    "# Process each file\n",
    "for file_path in tqdm(file_paths):\n",
    "    # Extract file name\n",
    "    file_name = file_path.split(\"/\")[-1].strip().split(\".\")[0].strip()\n",
    "    \n",
    "    try:\n",
    "        # Read sample to determine column count\n",
    "        sample_df = pd.read_csv(file_path, header=None, skiprows=10, nrows=1)\n",
    "        num_columns = len(sample_df.columns)\n",
    "\n",
    "        # Read full dataset\n",
    "        matrix_df = pd.read_csv(\n",
    "            file_path,\n",
    "            header=None,\n",
    "            skiprows=10,           # skip Excel rows 1–10\n",
    "            usecols=range(num_columns)  # use all available columns\n",
    "        )\n",
    "\n",
    "        # Convert to numpy array\n",
    "        matrix = matrix_df.to_numpy()\n",
    "        \n",
    "        # Convert to CSR sparse matrix format\n",
    "        sparse_matrix = sparse.csr_matrix(matrix)\n",
    "        \n",
    "        # Save to compressed NPZ file\n",
    "        output_path = os.path.join(output_directory, f\"{file_name}.npz\")\n",
    "        sparse.save_npz(output_path, sparse_matrix)\n",
    "        \n",
    "        # Optionally print statistics\n",
    "        # print(f\"Processed {file_name}, shape: {matrix.shape}, \" \n",
    "        #       f\"density: {sparse_matrix.nnz/(matrix.shape[0]*matrix.shape[1]):.4f}, \"\n",
    "        #       f\"non-zeros: {sparse_matrix.nnz}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11520/11520 [11:24<00:00, 16.84it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path to the directory\n",
    "directory = '/workspace/data/radar/24'\n",
    "output_directory = \"/workspace/data/radar_quantized/24_quantized\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Get all files in the directory\n",
    "file_paths = glob.glob(os.path.join(directory, '*'))\n",
    "\n",
    "# Process each file\n",
    "for file_path in tqdm(file_paths):\n",
    "    # Extract file name\n",
    "    file_name = file_path.split(\"/\")[-1].strip().split(\".\")[0].strip()\n",
    "    \n",
    "    try:\n",
    "        # Read sample to determine column count\n",
    "        sample_df = pd.read_csv(file_path, header=None, skiprows=10, nrows=1)\n",
    "        num_columns = len(sample_df.columns)\n",
    "\n",
    "        # Read full dataset\n",
    "        matrix_df = pd.read_csv(\n",
    "            file_path,\n",
    "            header=None,\n",
    "            skiprows=10,           # skip Excel rows 1–10\n",
    "            usecols=range(num_columns)  # use all available columns\n",
    "        )\n",
    "\n",
    "        # Convert to numpy array\n",
    "        matrix = matrix_df.to_numpy()\n",
    "        \n",
    "        # Create our own CSR representation\n",
    "        data = []          # Will hold non-zero values\n",
    "        indices = []       # Will hold column indices\n",
    "        indptr = [0]       # Row pointers (starting with 0)\n",
    "        \n",
    "        # Process each row\n",
    "        for row in matrix:\n",
    "            # Find non-zero elements in this row\n",
    "            row_indices = np.nonzero(row)[0]\n",
    "            row_data = row[row_indices]\n",
    "            \n",
    "            # Add this row's data and indices\n",
    "            data.extend(row_data.tolist())\n",
    "            indices.extend(row_indices.tolist())\n",
    "            \n",
    "            # Update indptr (points to end of this row/start of next)\n",
    "            indptr.append(len(data))\n",
    "        \n",
    "        # Create JSON representation\n",
    "        sparse_json = {\n",
    "            \"data\": data,\n",
    "            \"indices\": indices,\n",
    "            \"indptr\": indptr,\n",
    "            \"shape\": list(matrix.shape)\n",
    "        }\n",
    "        \n",
    "        # Save to JSON file\n",
    "        output_path = os.path.join(output_directory, f\"{file_name}.json\")\n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump(sparse_json, f)\n",
    "        \n",
    "        # Calculate and print statistics\n",
    "        density = len(data) / (matrix.shape[0] * matrix.shape[1])\n",
    "        # print(f\"Processed {file_name}, shape: {matrix.shape}, \" \n",
    "        #       f\"density: {density:.4f}, \"\n",
    "        #       f\"non-zeros: {len(data)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11520/11520 [08:00<00:00, 23.96it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Path to directories\n",
    "directory = '/workspace/data/radar/24'\n",
    "output_directory = \"/workspace/data/radar_quantized/24_quantized\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Get all files\n",
    "file_paths = glob.glob(os.path.join(directory, '*'))\n",
    "\n",
    "for file_path in tqdm(file_paths):\n",
    "    file_name = file_path.split(\"/\")[-1].strip().split(\".\")[0].strip()\n",
    "    \n",
    "    try:\n",
    "        # Read data as before\n",
    "        sample_df = pd.read_csv(file_path, header=None, skiprows=10, nrows=1)\n",
    "        num_columns = len(sample_df.columns)\n",
    "        \n",
    "        matrix_df = pd.read_csv(\n",
    "            file_path,\n",
    "            header=None,\n",
    "            skiprows=10,\n",
    "            usecols=range(num_columns)\n",
    "        )\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        matrix = matrix_df.to_numpy()\n",
    "        \n",
    "        # Find non-zero indices\n",
    "        indices = np.nonzero(matrix)\n",
    "        values = matrix[indices[0], indices[1]]\n",
    "        \n",
    "        # Convert to PyTorch sparse tensor\n",
    "        # First, create indices tensor (2 x nnz format)\n",
    "        i = torch.LongTensor(np.vstack(indices))\n",
    "        \n",
    "        # Then create values tensor\n",
    "        v = torch.FloatTensor(values)\n",
    "        \n",
    "        # Create sparse tensor\n",
    "        sparse_tensor = torch.sparse_coo_tensor(\n",
    "            i, v, torch.Size(matrix.shape)\n",
    "        )\n",
    "        \n",
    "        # Optional: Convert to CSR format (more efficient for some operations)\n",
    "        # sparse_tensor = sparse_tensor.to_sparse_csr()\n",
    "        \n",
    "        # Save to file (very efficient)\n",
    "        output_path = os.path.join(output_directory, f\"{file_name}.pt\")\n",
    "        torch.save(sparse_tensor, output_path)\n",
    "        \n",
    "        # print(f\"Processed {file_name}, shape: {matrix.shape}, non-zeros: {len(values)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
